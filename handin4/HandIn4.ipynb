{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning hand-in 4\n",
    "\n",
    "This handin is about implementing and using representative-based clustering\n",
    "algorithms.\n",
    "\n",
    "The handin is mandatory, and should be done groups of 2-3 students. Each group\n",
    "must prepare a report in PDF format as outlined below. Please submit all your\n",
    "Python files in a zip file, and your PDF report outside the zip file, to\n",
    "Blackboard no later than **Monday, December 19 at 9:00 AM**.\n",
    "\n",
    "All data and Python files are available below.\n",
    "\n",
    "For questions and issues regarding this hand-in, please use the\n",
    "[course discussion forum](https://bb.au.dk/webapps/blackboard/content/launchLink.jsp?course_id=_54703_1&toc_id=_830772_1).\n",
    "If you have problems that for some reason cannot be shared\n",
    "on the discussion forum, contact the teaching assistant, Mathias Rav, either\n",
    "by coming to his office, Nygaard 334, or by sending an email to [rav@cs.au.dk](mailto:rav@cs.au.dk)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# oh so pretty!\n",
    "def float_formatter(x):\n",
    "    if x < 0.001 and x > 0:\n",
    "        return \"   >0\" \n",
    "    elif x == 0:\n",
    "        return \"    0\" \n",
    "    else:\n",
    "        return \"%.3f\" % x\n",
    "np.set_printoptions(formatter={'float_kind':float_formatter})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_matrix(x, y, group, fmt='.', **kwargs):\n",
    "    \"\"\"\n",
    "    Given two d-dimensional datasets of n points,\n",
    "    makes a figure containing d x d plots, where the (i, j) plot\n",
    "    plots the ith dimension against the jth dimension.\n",
    "    \"\"\"\n",
    "\n",
    "    x = np.asarray(x)\n",
    "    y = np.asarray(y)\n",
    "    group = np.squeeze(np.asarray(group))\n",
    "    n, p = x.shape\n",
    "    n_, q = y.shape\n",
    "    n__, = group.shape\n",
    "    assert n == n_ == n__\n",
    "    groups = sorted(set(group))\n",
    "    if isinstance(fmt, str):\n",
    "        fmt = {k: fmt for k in groups}\n",
    "    fig, axes = plt.subplots(p, q, squeeze=False, **kwargs)\n",
    "    for i, axrow in enumerate(axes):\n",
    "        for j, ax in enumerate(axrow):\n",
    "            for g in groups:\n",
    "                ax.plot(x[group == g, i], y[group == g, j], fmt[g])\n",
    "            if len(axes) > 2:\n",
    "                ax.locator_params(tight=True, nbins=4)\n",
    "\n",
    "def plot_groups(x, group, fmt='.', **kwargs):\n",
    "    \"\"\"\n",
    "    Helper function for plotting a 2-dimensional dataset with groups\n",
    "    using plot_matrix.\n",
    "    \"\"\"\n",
    "    n, d = x.shape\n",
    "    assert d == 2\n",
    "    x1 = x[:, 0].reshape(n, 1)\n",
    "    x2 = x[:, 1].reshape(n, 1)\n",
    "    plot_matrix(x1, x2, group, fmt, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data sets\n",
    "\n",
    "In this hand-in, you will work with two different data sets: The Iris data set (for clustering) and two images (for compression)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iris data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Iris data set is included in sklearn as the `load_iris` function in the\n",
    "`sklearn.datasets` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn.datasets\n",
    "iris = sklearn.datasets.load_iris()\n",
    "data = iris['data']\n",
    "labels = iris['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Iris data set consists of 150 four-dimensional points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The labels are integers between 0 and 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "set(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way of visualizing the four-dimensional point set is by projecting it down to two dimensions, for instance by picking the second and third dimension of the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_2d = data[:, 1:3]\n",
    "plot_groups(data_2d, labels, {0: 'o', 1: 's', 2: '^'}, figsize=(4, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also make all 16 different plots of this kind, that is, we can make 4 × 4 plots where the (*i*, *j*)th plot displays axis *i* of the data set against axis *j*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_matrix(data, data, labels, {0: 'o', 1: 's', 2: '^'}, figsize=(12, 12))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A third way of dealing with the data set in just two dimensions is by applying **principal component analysis** (PCA) as discussed in class.\n",
    "\n",
    "Luckily, sklearn contains a class to compute exactly this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sklearn.datasets\n",
    "import sklearn.decomposition\n",
    "\n",
    "# Load Iris data set\n",
    "iris = sklearn.datasets.load_iris()\n",
    "data = iris['data']\n",
    "labels = iris['target']\n",
    "# Apply PCA\n",
    "pca = sklearn.decomposition.PCA(2)\n",
    "data_pca = pca.fit_transform(data)\n",
    "# Plot\n",
    "plot_groups(data_pca, labels, {0: 'o', 1: 's', 2: '^'}, figsize=(4, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image compression data\n",
    "\n",
    "For the image compression part of this hand-in, you will use the following two images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scipy.misc\n",
    "\n",
    "def download_image(url):\n",
    "    filename = url[url.rindex('/')+1:]\n",
    "    try:\n",
    "        with open(filename, 'rb') as fp:\n",
    "            return scipy.misc.imread(fp) / 255\n",
    "    except FileNotFoundError:\n",
    "        import urllib.request\n",
    "        with open(filename, 'w+b') as fp, urllib.request.urlopen(url) as r:\n",
    "            fp.write(r.read())\n",
    "            return scipy.misc.imread(fp) / 255\n",
    "\n",
    "img_facade = download_image('https://users-cs.au.dk/rav/ml/handins/h4/nygaard_facade.jpg')\n",
    "img_stairs = download_image('https://users-cs.au.dk/rav/ml/handins/h4/nygaard_stairs.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.imshow(img_facade)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.imshow(img_stairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks\n",
    "\n",
    "### Implementing EM and the K-means algorithm\n",
    "\n",
    "You must implement the K-means algorithm and the Gaussian Mixture Expectation\n",
    "Maximization algorithm as discussed on p. 349 and p. 335 of the textbook [ZM].\n",
    "For the EM algorithm, you may use the `pdf` function below to compute the probability\n",
    "densities in the Gaussian Mixture model.\n",
    "\n",
    "You should use the Python code displayed previously to load and display Iris data\n",
    "and apply PCA to reduce it from four dimensions to two.\n",
    "\n",
    "Use the 2d Iris data to validate your algorithms (compare the results you get\n",
    "with the results in the textbook on the same data), and run your algorithms on\n",
    "the 4d data and compare."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code structure\n",
    "\n",
    "You are allowed to structure your code however you want, but you may want to use the following guidelines to get started.\n",
    "\n",
    "You can structure your implementation of the K-means algorithm as follows.\n",
    "First, you should implement a function `closest` that takes an `n × d` data matrix and a `k × d` matrix of centers and computes for each data point the closest center."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def closest(data, centers):\n",
    "    n, d = data.shape\n",
    "    k, d_ = centers.shape\n",
    "    assert d == d_\n",
    "    \n",
    "    # Insert your code here\n",
    "    dist = np.zeros([n,k])\n",
    "    for kk in range(k):\n",
    "        center = centers[kk,:]\n",
    "        for nn in range(n):\n",
    "            dist[nn,kk] = np.sqrt(np.sum((data[nn,:]-center)**2))\n",
    "    rep = np.argmin(dist, axis=1)\n",
    "    \n",
    "    # rep should contain a representative index for each data point\n",
    "    assert rep.shape == (n,)\n",
    "    assert np.all((0 <= rep) & (rep < k))\n",
    "    return rep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should implement a function that implements the K-means cost function, that is, a function that takes an `n × d` data matrix, a `k × d` matrix of centers, and a representative array mapping each data point to a center."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def kmeans_cost(data, centers, rep):\n",
    "    n, d = data.shape\n",
    "    k, d_ = centers.shape\n",
    "    assert d == d_\n",
    "    assert rep.shape == (n,)\n",
    "\n",
    "    # Insert your code here\n",
    "    data_rep = centers[rep]\n",
    "    cost = np.sum(np.sqrt(np.sum((data_rep-data)**2)))\n",
    "\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test kmeans_cost: (Martin)\n",
    "assert kmeans_cost(data,data,closest(data,data)) == 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, you should implement the K-means algorithm described in class. You should have a stopping parameter `epsilon` and stop when the centers move less than `epsilon` in an iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def kmeans(data, k, epsilon):\n",
    "    data = np.asarray(data)\n",
    "    n, d = data.shape\n",
    "\n",
    "    # Initialize centers\n",
    "    # randomly chosen points from data is used as centers (same datapoint)\n",
    "    centers = data[np.random.choice(range(n), size=k, replace=False),:]\n",
    "\n",
    "    tired = False\n",
    "    old_centers = np.zeros_like(centers)\n",
    "    while not tired:\n",
    "        old_centers[:] = centers\n",
    "\n",
    "        rep = closest(data, centers)\n",
    "        for kk in range(k):\n",
    "            if any(rep==kk):\n",
    "                centers[kk,:] = np.average(data[rep==kk,:], axis=0)\n",
    "        dist = np.sqrt(((centers - old_centers) ** 2).sum(axis=1))\n",
    "        tired = np.max(dist) <= epsilon\n",
    "\n",
    "    return centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# test kmeans\n",
    "\n",
    "data = np.random.choice(range(800), size=300, replace=False).T.reshape(-1,2)\n",
    "print(data.shape)\n",
    "plt.figure()\n",
    "plt.plot(data[:,0],data[:,1], '*')\n",
    "\n",
    "clusters = kmeans(data,10,0.0000001)\n",
    "print(clusters)\n",
    "plt.plot(clusters[:,0],clusters[:,1], 'ro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For your implementation to the Gaussian Mixture exercise, you may want to use the following two helper functions.\n",
    "\n",
    "The first function takes a description of a Gaussian Mixture (that is, the mean, covariance matrix and prior of each Gaussian) and returns the probability densities of each point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.stats import multivariate_normal\n",
    "def pdf(points, mean, cov, prior):\n",
    "    points, mean, cov = np.asarray(points), np.asarray(mean), np.asarray(cov)\n",
    "    prior = np.asarray(prior)\n",
    "    n, d = points.shape\n",
    "    k, d_1 = mean.shape\n",
    "    k_2, d_2, d_3 = cov.shape\n",
    "    k_3, = prior.shape\n",
    "    assert d == d_1 == d_2 == d_3\n",
    "    assert k == k_2 == k_3, \"%s %s %s should be equal\" % (k, k_2, k_3)\n",
    "\n",
    "    # Compute probabilities\n",
    "    prob = []\n",
    "    for i in range(k):\n",
    "        if prior[i] < 1 / k ** 3:\n",
    "            prob.append(np.zeros(n))\n",
    "        else:\n",
    "            prob.append(\n",
    "                prior[i] *\n",
    "                multivariate_normal.pdf(\n",
    "                    mean=mean[i], cov=cov[i], x=points, allow_singular=True))\n",
    "    prob = np.transpose(prob)  # n x k\n",
    "    # Normalize cluster probabilities of each point\n",
    "    prob = prob / np.sum(prob, axis=1, keepdims=True)  # n x k\n",
    "\n",
    "    assert prob.shape == (n, k)\n",
    "    assert np.allclose(prob.sum(axis=1), 1)\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following helper function computes the most likely class of each point under a given Gaussian Mixture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def most_likely(points, mean, cov, prior):\n",
    "    prob = pdf(points, mean, cov, prior)\n",
    "    return np.argmax(prob, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When implementing the Gaussian Mixture E-M algorithm, you may want to start out with the following code structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def scale(points, mean, cov):\n",
    "    scale =  np.identity(points.shape[1]) * 1/cov.max()\n",
    "    cov_scale = [scale]*cov.shape[0]\n",
    "    points = (scale @ points.T).T\n",
    "    mean = (scale @ mean.T).T\n",
    "    cov = scale @ cov @ scale\n",
    "    return points, mean, cov\n",
    "\n",
    "def scaleMean(mean, scale, scale_min):\n",
    "    return (scale @ (mean-scale_min).T).T\n",
    "\n",
    "def em(points, k, epsilon, mean=None, pad=0.0, maxiter = None, log = False):\n",
    "    log_data=None\n",
    "    if log == True and maxiter is not None:\n",
    "        log_data = np.zeros(maxiter+1)\n",
    "    points = np.asarray(points)\n",
    "    n, d = points.shape\n",
    "\n",
    "    # Initialize and validate mean\n",
    "    if mean is None:\n",
    "        # Randomly pick k points\n",
    "        mean = points[np.random.choice(range(n), size=k, replace=False)]\n",
    "\n",
    "    # Validate input\n",
    "    mean = np.asarray(mean)\n",
    "    k_, d_ = mean.shape\n",
    "    assert k == k_\n",
    "    assert d == d_\n",
    "\n",
    "    # Initialize cov, prior\n",
    "    cov = np.asarray([np.identity(d)]*k)\n",
    "    \n",
    "    \n",
    "    prior = np.ones(shape=(k))/k\n",
    "\n",
    "    tired = False\n",
    "    old_mean = np.zeros_like(mean)\n",
    "    Em = np.log(0)\n",
    "    q = 0\n",
    "    \n",
    "    while not tired:\n",
    "        old_mean[:] = mean\n",
    "\n",
    "        # Expectation step\n",
    "        sPoints, sMean, sCov = scale(points, mean, cov)\n",
    "        px_c = pdf(points, mean, cov, prior)\n",
    "        px = (prior * px_c).sum(axis=1)\n",
    "        pc_x = prior[np.newaxis, :] * ( px_c / px[:, np.newaxis])\n",
    "        \n",
    "        # Maximization step\n",
    "        # calculate weights\n",
    "        #We believe that any category must contain at least 10% of the points\n",
    "\n",
    "        # We calculate prior, and do some padding. The idea is, that we believe that X% of the data must be\n",
    "        # distributed evently across our classes. So we can always select some X% percent of our datapoints,\n",
    "        # which will be evently distributed. This is pretty much the same as pseudocounting.\n",
    "        prior = ((pc_x.sum(axis = 0)) / n)*(1-pad) + pad/k\n",
    "        \n",
    "        \n",
    "        # calculate mean\n",
    "        mean = np.nan_to_num(((pc_x.T @ points).T/pc_x.sum(axis=0)).T)\n",
    "        pointsMinusMean = np.transpose((points[:, :, np.newaxis]-mean.T), (2, 0, 1))[:, :, :, np.newaxis]\n",
    "        pointsMinusMean_T = np.transpose(pointsMinusMean, (0, 1, 3, 2))\n",
    "        cov = np.nan_to_num((pc_x.T[:, :, np.newaxis, np.newaxis] \n",
    "               * (pointsMinusMean @ pointsMinusMean_T)).sum(axis=1)/pc_x.sum(axis=0)[:, np.newaxis, np.newaxis])\n",
    "        \n",
    "        # Finish condition\n",
    "        Em_ = np.log(px).sum(axis=0)\n",
    "        dist = np.absolute(Em-Em_)\n",
    "        Em = Em_\n",
    "        if (q % 10 == 0):\n",
    "            print(\"\\rdist is \" + str(dist) + \" after \" + str(q) + \" iterations\", end=\"\")\n",
    "        tired = dist < epsilon\n",
    "        if maxiter != None:\n",
    "            if log == True:\n",
    "                log_data[q] = dist\n",
    "            tired = q >= maxiter\n",
    "        q += 1\n",
    "\n",
    "    # Validate output\n",
    "    #print(\"\\rFinal dist was: \" + str(dist) + \" after \" + str(q) + \"iterations.\")\n",
    "    assert mean.shape == (k, d)\n",
    "    assert cov.shape == (k, d, d)\n",
    "    assert prior.shape == (k,)\n",
    "    return mean, cov, prior, log_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vote_labels(guessed, actual):\n",
    "    respons = np.zeros_like(guessed)\n",
    "    for l in np.unique(guessed):\n",
    "        vl = np.argmax(np.bincount(actual[guessed == l]))\n",
    "        respons[guessed == l] = vl\n",
    "    return respons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# test em\n",
    "\n",
    "import sklearn.datasets\n",
    "iris = sklearn.datasets.load_iris()\n",
    "data = iris['data']\n",
    "labels = iris['target']\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "mean, cov, prior, log_data = em(data, 3, 0.1, pad=0.3)#, maxiter=10000, log=True)\n",
    "guessed_labels = most_likely(data, mean, cov, prior)\n",
    "voted_labels = vote_labels(guessed_labels, labels)\n",
    "plot_matrix(data, data, voted_labels, {0: 'o', 1: 's', 2: '^'}, figsize=(12, 12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost function for the K-means problem\n",
    "\n",
    "To ensure that your implementation of the K-means algorithm is correct, you\n",
    "should implement the cost function (objective function / optimization goal)\n",
    "for the K-means problem. Print the cost after each iteration of the K-means\n",
    "algorithm -- the cost should be decreasing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating clusterings\n",
    "\n",
    "Implement the F1 score (build the contingency table p. 426, measure pp. 427-428)\n",
    "and the silhouette coefficient (pp. 444-445), and compare the quality of\n",
    "several runs of your algorithms with different values for *k*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def f1(predicted, labels):\n",
    "    n, = predicted.shape\n",
    "    assert labels.shape == (n,)\n",
    "    r = np.max(predicted) + 1\n",
    "    k = np.max(labels) + 1\n",
    "\n",
    "    # Implement the F1 score here\n",
    "    contingency = np.zeros((r, k))\n",
    "    for i in range(r):\n",
    "        for j in range(k):\n",
    "            contingency[i,j] = np.sum(labels[predicted == i] == j)\n",
    "            \n",
    "    prec = np.zeros(r)\n",
    "    prec = np.nan_to_num(contingency.max(axis=1)/contingency.sum(axis=1))\n",
    "    recall = np.zeros(r)\n",
    "    i_max = contingency.argmax(axis=1)\n",
    "    recall = np.nan_to_num(contingency.max(axis=1)/contingency.sum(axis=0))[i_max]\n",
    "    F_individual = 2*prec*recall/(prec+recall)\n",
    "    F_overall = F_individual.sum()/r\n",
    "\n",
    "    assert contingency.shape == (r, k)\n",
    "    return F_individual, F_overall, contingency\n",
    "\n",
    "def silhouette(data, predicted):\n",
    "    data = np.asarray(data)\n",
    "    n, d = data.shape\n",
    "    predicted = np.squeeze(np.asarray(predicted))\n",
    "    k = np.max(predicted) + 1\n",
    "    assert predicted.shape == (n,)\n",
    "\n",
    "    # Implement the computation of the silhouette coefficient for each data point here.\n",
    "    s = ... #individual, average\n",
    "\n",
    "    assert s.shape == (n,)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#### import sklearn.datasets\n",
    "iris = sklearn.datasets.load_iris()\n",
    "data = iris['data']\n",
    "labels = iris['target']\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "\n",
    "repetitions = 3\n",
    "kays = range(1,11)\n",
    "label_set = set(labels)\n",
    "err_free = True\n",
    "\n",
    "KMresF1 = np.zeros([len(kays), (1+len(label_set)), repetitions])\n",
    "EMresF1 = np.zeros([len(kays), (1+len(label_set)), repetitions])\n",
    "KMresSil = np.zeros([len(kays), (1+len(label_set)), repetitions])\n",
    "EMresSil = np.zeros([len(kays), (1+len(label_set)), repetitions])\n",
    "\n",
    "if True:\n",
    "    for kay_idx, kay in enumerate(kays):\n",
    "        ## K-means\n",
    "        for rep in range(repetitions):\n",
    "            try:\n",
    "                means = kmeans(data, kay, 0.0000001)\n",
    "                guessed_labels = closest(data, means)\n",
    "                voted_labels = vote_labels(guessed_labels, labels)\n",
    "                # F1\n",
    "                res0, res1, res2 = f1(voted_labels, labels)\n",
    "                KMresF1[kay_idx,0:len(res0),rep] = np.asarray(res0)\n",
    "                KMresF1[kay_idx,-1,rep] = res1\n",
    "            except:\n",
    "                #print(\"røv1\", kay)\n",
    "                err_free = False\n",
    "            try:\n",
    "                #Silhouette\n",
    "                res0, res1, res2 = silhouette(data, voted_labels)\n",
    "                KMresSil[kay_idx,0:len(res0),rep] = np.asarray(res0)\n",
    "                KMresSil[kay_idx,-1,rep] = res1\n",
    "            except:\n",
    "                #print(\"røv2\", kay)\n",
    "                err_free = False\n",
    "\n",
    "            ## EM\n",
    "            try: #because singular_matrix errors is a thing \n",
    "                mean, cov, prior = em(data, kay, 0.000001, means)\n",
    "                guessed_labels = most_likely(data, mean, cov, prior)\n",
    "                voted_labels = vote_labels(guessed_labels, labels)\n",
    "                # F1\n",
    "                res0, res1, res2 = f1(voted_labels, labels)\n",
    "                EMresF1[kay_idx,0:len(res0),rep] = np.asarray(res0)\n",
    "                EMresF1[kay_idx,-1,rep] = res1\n",
    "            except:\n",
    "                #print(\"røv3\", kay)\n",
    "                err_free = False\n",
    "            try:\n",
    "                #Silhouette\n",
    "                res0, res1, res2 = silhouette(data, voted_labels)\n",
    "                EMresSil[kay_idx,0:len(res0),rep] = np.asarray(res0)\n",
    "                EMresSil[kay_idx,-1,rep] = res1\n",
    "                #print(\"røv4\", kay)\n",
    "                err_free = False\n",
    "\n",
    "\n",
    "def plot_measure(results, kays, err, colors, labels, ax=None):\n",
    "    pad = np.linspace(-0.3, 0.3, num=len(colors))\n",
    "    if ax==None:\n",
    "        fig, ax, _ = plt.figure()\n",
    "    for iii in range(len(colors)):\n",
    "        assert err[:,iii].shape == results[:,iii].shape\n",
    "        #ax.errorbar(kays, results[:,iii], colors[iii], label=labels[iii], yerr=err[:,iii], fmt='o')\n",
    "        ax.errorbar(kays+pad[iii], results[:,iii], yerr=err[:,iii], fmt='o')\n",
    "\n",
    "    return\n",
    "\n",
    "\n",
    "#ax.errorbar(x, y, yerr=yerr, fmt='o')\n",
    "\n",
    "cluster_colors = ['ob', 'sg', '^r', 'k']\n",
    "cluster_labels = ['Cluster a', 'Cluster b', 'Cluster c', 'Average']\n",
    "\n",
    "# # ax.errorbar(x, y, yerr=[yerr, 2*yerr], xerr=[xerr, 2*xerr], fmt='--o')\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(nrows=2, ncols=2, sharex=True, sharey=True, figsize=(12,5))\n",
    "\n",
    "plot_measure(np.average(KMresF1,  axis=2), kays, np.std(KMresF1,  axis=2), cluster_colors, cluster_labels, axs[0,0])\n",
    "plot_measure(np.average(EMresF1,  axis=2), kays, np.std(EMresF1,  axis=2), cluster_colors, cluster_labels, axs[0,1])\n",
    "plot_measure(np.average(KMresSil, axis=2), kays, np.std(KMresSil, axis=2),cluster_colors, cluster_labels, axs[1,0])\n",
    "plot_measure(np.average(EMresSil, axis=2), kays, np.std(EMresSil, axis=2),cluster_colors, cluster_labels, axs[1,1])\n",
    "\n",
    "axs[0,0].set_title(\"K-means clustering\")\n",
    "axs[0,1].set_title(\"GM-Expectations Maximization clustering\")\n",
    "axs[0,0].set_ylabel(\"F1 score\")\n",
    "axs[1,0].set_ylabel(\"Silhouette measure\")\n",
    "axs[1,0].set_xlabel(\"Number of clusters, K\")\n",
    "axs[1,1].set_xlabel(\"Number of clusters, K\")\n",
    "axs[1,0].legend(loc = 'lower center', bbox_to_anchor = (0,-0.1,1,1),\n",
    "            bbox_transform = plt.gcf().transFigure, ncol=4)\n",
    "plt.tight_layout()\n",
    "\n",
    "assert err_free"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing EM\n",
    "\n",
    "In order to determine an initial set of cluster centers for EM, one can\n",
    "utilize the best centroids determined by k-means. For this, run k-means\n",
    "several times, and choose the best one. You are asked to do this for the 2d\n",
    "and 4d IRIS data set and to compare with your earlier results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image compression\n",
    "\n",
    "You are provided with two images that you are asked to subject to clustering\n",
    "in order to find the most representative colors for the two images. Use these\n",
    "results to display a compressed version of the images.\n",
    "\n",
    "You may use a function like the following to run K-means on an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def compress_kmeans(im, k=4):\n",
    "    height, width, depth = im.shape\n",
    "\n",
    "    data = im.reshape((height * width, depth))\n",
    "    centers = kmeans(data, k, 1e-2)\n",
    "    rep = closest(data, centers)\n",
    "    data_compressed = centers[rep]\n",
    "\n",
    "    im_compressed = data_compressed.reshape((height, width, depth))\n",
    "    plt.figure()\n",
    "    plt.imshow(im_compressed)\n",
    "    plt.show()\n",
    "\n",
    "def compress_facade(k=4):\n",
    "    img_facade = download_image('https://users-cs.au.dk/rav/ml/handins/h4/nygaard_facade.jpg')\n",
    "    compress_kmeans(img_facade, k=k)\n",
    "\n",
    "def compress_stairs(k=4):\n",
    "    img_stairs = download_image('https://users-cs.au.dk/rav/ml/handins/h4/nygaard_stairs.jpg')\n",
    "    compress_kmeans(img_stairs, k=k)\n",
    "    \n",
    "compress_facade()\n",
    "compress_stairs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Report\n",
    "\n",
    "Your report should be no more than 3 pages and clearly state who is in the\n",
    "group. It must cover:\n",
    "\n",
    "* The status of the work, i.e., does it work, if not, then why.\n",
    "* A discussion of plots of at least two runs of your algorithm\n",
    "  implementations detailing what you can see. Make sure that you relate this\n",
    "  to the discussion in the lecture or textbook about the strengths and\n",
    "  weaknesses of the algorithms.\n",
    "* A discussion of plots of the evaluation measures F1 and silhouette\n",
    "  coefficient, detailing what you can learn from them. Include an explanation\n",
    "  of what the evaluation measures reflect.\n",
    "* Describe how you can use one of the clustering algorithms for image\n",
    "  compression, and demonstrate the results for at least one algorithm on both\n",
    "  images, discussing their quality and giving a reasoning for the differences."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
