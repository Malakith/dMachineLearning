{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OCR and Logistic Regression\n",
    "In this handin you will build classifiers for optical character\n",
    "recognition (OCR). You should use Python 3 (preferably 3.5) and NumPy for the\n",
    "programming parts. The models can be implemented with relatively few\n",
    "lines of actual code if expressed using matrices and vectors. Python\n",
    "and NumPy are **much much** faster when you express \n",
    "computation using vectorized code and matrix products instead of for-loops,\n",
    "so the key is in understanding matrix products. \n",
    "\n",
    "In this document we go through the models in quite a lot of detail to\n",
    "ease the understanding. We use matrix and vector notation throughout\n",
    "since it simplifies the exposition, so you might as well get used to it.\n",
    "In mathematical formulas we shall think of all our vectors being column vectors.\n",
    "\n",
    "The exact deliverables are stated after the exercise descriptions.\n",
    "\n",
    "### Formalities\n",
    "\n",
    "You can write your final report in Danish if you prefer.  The maximal\n",
    "report length is 5 pages. You are allowed to be up to 3 members in a\n",
    "group.  You are encouraged to discuss the exercise between groups and\n",
    "help each other as much as possible <b>without of course copying each\n",
    "other's work</b>. Particularly, discussing the quality of your classifiers\n",
    "is probably a good idea to get an indication of whether you are doing it\n",
    "correctly. For these discussions and additional questions use the\n",
    "discussion forum on the Blackboard course site.\n",
    "Bonus questions may be skipped, but you are encouraged to try and at least think\n",
    "about answering them.\n",
    "\n",
    "\n",
    "### Important remarks\n",
    "* You may run into numerical issues. Deal with them...\n",
    "\n",
    "* Note that running the experiments may be rather slow if you have not been able to optimize your code properly, so start running experiments early -- don't wait until the day before the deadline!\n",
    "\n",
    "# The data\n",
    "First, let's see how we load the data and visualize it.\n",
    "There are two data sets. The famous MNIST-Digits data set is a classic machine learning data set, and the AU-Digits data set generated by you and your predecessor Machine Learning students. Each data set is split in two: the training and the test set.\n",
    "\n",
    "We released the AU Digits data set on September 8 2016, so that is what we focus on for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "\n",
    "# Change this to the AU digits data set when it has been released!\n",
    "train_file = np.load('auTrain.npz')\n",
    "print(train_file.keys())  # ['digits', 'labels']\n",
    "images = train_file['digits'][:10]\n",
    "labels = train_file['labels'][:10]\n",
    "print('Shape of input data: %s' % (images.shape,))\n",
    "print('Shape of input labels: %s' % (labels.shape,))\n",
    "\n",
    "# Change this to the AU digits data set when it has been released!\n",
    "test_file = np.load('auTest.npz')\n",
    "print(test_file.keys())\n",
    "images_test = test_file['digits']\n",
    "print('Shape of test input data: %s' % (images_test.shape,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With some of the data loaded, let's visualize some of the input images.\n",
    "The same code can be useful later for visualizing the weight vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# show the first 16 digits from training set\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = images[0:16, :].reshape(-1, 28, 28)\n",
    "x = x.transpose(1, 0, 2)\n",
    "plt.imshow(x.reshape(28, -1), cmap='bone')\n",
    "plt.yticks([])\n",
    "plt.xticks([])\n",
    "plt.figure()\n",
    "# show the first 16 twos from training set\n",
    "idx1 = (labels == 2)\n",
    "img2s = images[idx1, :]\n",
    "x2 = img2s[0:16, :].reshape(-1, 28, 28)\n",
    "x2 = x2.transpose(1, 0, 2)\n",
    "plt.imshow(x2.reshape(28, -1), cmap='bone')\n",
    "plt.yticks([])\n",
    "plt.xticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background on Logistic Regression\n",
    "\n",
    "In logistic regression we model the target function as a probability distribution\n",
    "$p(y\\mid x)$.\n",
    "This probability distribution is defined by the logistic function\n",
    "$\\sigma(z) = 1/(1+e^{-z})$\n",
    "over a linear function of the input data,\n",
    "$z = \\sum_{i=0}^d \\theta_i x_i = \\theta^\\intercal x$,\n",
    "that is parameterized by the vector $\\theta$.\n",
    "As in the lectures and the first coding exercise,\n",
    "we encode the bias variable into the input vectors as $x_0$ and force\n",
    "$x_0 = 1$ on all data points (Note that you have amend the input data with the all ones column)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "x = np.linspace(-10, 10, 200)\n",
    "plt.plot(x, sigmoid(x), 'r', linewidth=2)\n",
    "plt.axis([-10, 10, -0.1, 1.1])\n",
    "plt.title('Logistic function')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logistic function is a nice smooth function with simple\n",
    "derivaties,\n",
    "$\\frac{\\partial \\sigma }{\\partial z} = (1-\\sigma(z))\\sigma(z)$,\n",
    "which makes it pleasing to work with.\n",
    "\n",
    "The model becomes,\n",
    "\n",
    "$$\n",
    "p(y \\mid x, \\theta) = \\left \\{\n",
    "\\begin{array}{l l}\n",
    "  \\sigma(\\theta^\\intercal x)\n",
    "  & \\text{ if } y=1  \\\\\n",
    "  1 - \\sigma(\\theta^\\intercal x)\n",
    "  & \\text { if } y=0,\n",
    "\\end{array}\n",
    "\\right.\n",
    "$$\n",
    "\n",
    "that is, the probability that $y = 1$ given $x$ and $\\theta$\n",
    "is the probability of getting heads with\n",
    "a biased coin, where the bias is $\\sigma(\\theta^\\intercal x)$.\n",
    "Given a fixed $\\theta$ we can use the function $p$ to make classification by\n",
    "returning the most likely class.\n",
    "\n",
    "The input is a labeled dataset $D = (X, Y) = \\{(x_i, y_i) \\mid i = 1, \\dots, n \\}$, where $x_i \\in \\{1\\} \\times \\mathbb{R}^d$ is the (d+1)-dimensional data point (with the first dimension used for the bias 1)\n",
    "and $y_i \\in \\{0,1\\}$ is its label.\n",
    "\n",
    "Let us think of $X$ as an $n \\times (d+1)$ matrix, such that the data points are given as its rows.\n",
    "Let us also arrange the labels as a $n$-dimensional vector $y$.\n",
    "\n",
    "Let us stress now that the entries of $y_i$'s are not probabilities, but instead actual realisation of events.\n",
    "\n",
    "We assume (as always) that the points in $D$ are independently sampled from the unknown distribution.\n",
    "This means that under our model, for a fixed parameter vector $\\theta$, the likelihood of the data is precisely\n",
    "\n",
    "$$\n",
    "p(D \\mid \\theta)\n",
    "= \\prod_{(x,y)\\in D}\n",
    "  p(y \\mid x,\\theta)\n",
    "= \\prod_{(x,y)\\in D}\n",
    "  \\sigma(\\theta^\\intercal x )^{y}\n",
    "  (1-\\sigma(\\theta^\\intercal x))^{1-y}\n",
    "$$\n",
    "\n",
    "Notice the last equality, which gives a convenient way of expressing\n",
    "the probability $p(y \\mid x,\\theta)$ as a product.\n",
    "You should convince yourself that it is true; recall that $y$ is either 0 or 1.\n",
    "\n",
    "As derived in class the negative log likelihood (cross entropy) is:\n",
    "$$\n",
    "\\mathrm{NLL}(D\\mid \\theta)\n",
    "= - \\sum_{i=1}^n\n",
    "y_i \\ln(\\sigma(\\theta^\\intercal x_i)) +\n",
    "(1-y_i) \\ln(1-\\sigma(\\theta^\\intercal x_i))\n",
    "$$\n",
    "and the gradient (vector) is\n",
    "$$\n",
    "\\nabla \\mathrm{NLL}(D \\mid \\theta)\n",
    "= \\frac{\\partial \\mathrm{NLL}}{\\partial \\theta}\n",
    "= -X^\\intercal(Y-\\sigma(X\\theta))\n",
    "$$\n",
    "and we defined the cost function to be $E_\\textrm{in} = \\frac{1}{n} \\mathrm{NLL}$, which of course means $\\nabla E_\\textrm{in} = \\frac{1}{n} \\nabla \\mathrm{NLL}$.\n",
    "\n",
    "Your job is to implement a gradient descent algorithm that finds a good $\\theta$\n",
    "by minimizing the in-sample error (or negative log likelihood).\n",
    "We have provided a simple code structure for you to follow below.\n",
    "**You should make a small test set to work on initially to reduce test time during development.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background on gradient descent\n",
    "\n",
    "The basic gradient search (batch gradient descent) algorithm for minimizing a (convex) function $F(\\theta)$ is an iterative procedure that in each step updates the current solution by the equation\n",
    "$$\n",
    "\\theta = \\theta - \\eta \\nabla F(\\theta)\n",
    "$$\n",
    "where $\\eta$ is the *learning rate*, that is, the size of each step taken in gradient descent.\n",
    "\n",
    "With a sufficiently small learning rate $\\eta$ in each step (it can vary between iterations), we should see decreasing values of $F(\\theta)$ until we are (approximately) at the global minimum. On the other hand, with too small learning rate, the convergence might be too slow.\n",
    "\n",
    "### Stochastic gradient descent\n",
    "\n",
    "Since each step in basic gradient descent needs to consider the entire data set to compute the gradient, each step can be very slow. **Stochastic gradient descent** (SGD) is a much faster method that uses randomization to avoid looking at all the data points in each step.\n",
    "\n",
    "In SGD, our cost function $F$ must be an average over all the points, that is a function on the form $F(\\theta) = \\frac{1}{n} \\sum_{i = 1}^n F_i(\\theta)$. Notice that this is the case for logistic regression considered above.\n",
    "When the cost function is an average, the gradient of the cost becomes simply\n",
    "$\\nabla F(\\theta) = \\frac{1}{n} \\sum_{i=1}^n \\nabla F_i(\\theta) = E_{i}[\\nabla F_i(\\theta)]$.\n",
    "In SGD we estimate $\\nabla F(\\theta)$ by picking a random data point $i$ and computing $\\nabla F_i(\\theta)$. This is an unbiased estimator for $\\nabla F(\\theta)$ which we can use instead of $\\nabla F(\\theta)$ at each iteration.\n",
    "\n",
    "In practice we will never sample a random $i$ at each iteration, but instead we approximate the above method by running multiple so-called *epochs* where the data is processed in a random order.\n",
    "At the beginning of each epoch, we output the current true value of $F(\\theta)$, and we shuffle the data set before processing each data point as above.\n",
    "There are built-in fast vectorized shuffling methods in NumPy so you don't have to write your own shuffling procedure.\n",
    "\n",
    "### Minibatch stochastic gradient descent\n",
    "\n",
    "In SGD described above, the estimate of $\\nabla F(\\theta)$ has a high variance since it only considers a single data point $i$. The idea of minibatch SGD is to more than one sample from the dataset when estimating the gradient to decrease the variance while maintaining the advantage of each iteration being fast. Namely, we pick a minibatch size $k$ (typically between 10 and 50) and sample $i_1, i_2, \\dots, i_k$, and then we compute the better estimator (in terms of variance) $\\frac{1}{k} \\sum_{j=1}^k \\nabla F_{i_j}(\\theta)$ for $\\nabla F(\\theta)$. As for SGD, in practice we will approximate that by running epochs where the data is shuffled at the beginnning of each epoch, after which all datapoints are processed $k$ at a time in the shuffled order.\n",
    "\n",
    "While there exist sophisticated stop criteria to decide when we have run enough epochs,\n",
    "in our code below we take the simple approach of running for a fixed number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You rang sir/miss\n",
      "Shape of input data: (10380, 784) (10380,)\n",
      "    Start NLL:  1438.97354684\n",
      "batch_grad_descent\n",
      "    Final NLL:  0.000833424363639\n",
      "mini_batch_grad_descent\n",
      "          NLL:  1034.94521312 after  1  epochs.\n",
      "          NLL:  1374.57652675 after  2  epochs.\n",
      "          NLL:  884.70328353 after  3  epochs.\n",
      "          NLL:  1802.31904109 after  4  epochs.\n",
      "          NLL:  656.956864776 after  5  epochs.\n",
      "          NLL:  692.215741693 after  6  epochs.\n",
      "          NLL:  598.911598683 after  7  epochs.\n",
      "          NLL:  783.274424974 after  8  epochs.\n",
      "          NLL:  456.217957227 after  9  epochs.\n",
      "          NLL:  416.830561568 after  10  epochs.\n",
      "          NLL:  861.758892518 after  11  epochs.\n",
      "          NLL:  420.607679895 after  12  epochs.\n",
      "    Final NLL:  416.830561568\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def load_data():\n",
    "    \"\"\"Loads the training data from file\"\"\"\n",
    "    train_file = np.load('auTrain.npz')\n",
    "    images = train_file['digits']\n",
    "    labels = train_file['labels']\n",
    "    print('Shape of input data: %s %s' % (images.shape, labels.shape))\n",
    "    return images, labels\n",
    "\n",
    "def logistic(z):\n",
    "    \"\"\" \n",
    "    Computes the logistic function to each entry in input vector z.\n",
    "\n",
    "    Args:\n",
    "        z: A vector (numpy array) \n",
    "    Returns:\n",
    "        A new vector of the same length where each entry is the value of the \n",
    "        corresponding input entry after applying the logistic function to it\n",
    "    \"\"\"\n",
    "    \n",
    "    # Undgå overflow ved at begrænse z (exp(+/-710) -> overflow)\n",
    "    z = np.clip(z,-709,709)\n",
    "    return 1/(1 + np.exp(-z))\n",
    "\n",
    "def log_cost(X, y, w=None, reg=0):\n",
    "    \"\"\"\n",
    "    Compute the regularized cross entropy and the gradient under the logistic regression model \n",
    "    using data X,y and weight vector w\n",
    "\n",
    "    Args:\n",
    "        X: A 2D numpy array of data points stored row-wise (n x d)\n",
    "        y: A 1D numpy array of target values (array of length n)\n",
    "        w: A 1D numpy array of weights (d x 1)\n",
    "        reg: Optional regularization parameter\n",
    "    Returns:\n",
    "        cost: Average Negative Log Likelihood of w\n",
    "        grad: The gradient of the average Negative Log Likelihood at w\n",
    "    \"\"\"\n",
    "    if w is None:\n",
    "        w = np.zeros((X.shape[1], 1))\n",
    "    \n",
    "    # shape the targetvalues as a one-dimensional matrix\n",
    "    n, d = X.shape\n",
    "    y = y.reshape(n, 1)\n",
    "    pad = 0.000000000000001\n",
    "    # calculate the logistic function that will be used repeatedly\n",
    "    sigma = logistic(X.dot(w))\n",
    "    # Avoid using sigma's == 1 in NLL calculation (avoid log(0)'s)\n",
    "    cost = - np.sum( y*np.log(sigma+pad) + (1 - y)*np.log(1-sigma+pad) )\n",
    "    grad = - X.T.dot(y-logistic(X.dot(w)))\n",
    "    return cost, grad\n",
    "\n",
    "def batch_grad_descent(X, y, w=None, reg=0, stepsize=0.1, maxiter=2**10,):\n",
    "    \"\"\" \n",
    "    Run Batch Gradient Descent on data X,y to minimize the NLL for logistic regression on data X,y\n",
    "\n",
    "    Args:\n",
    "        X: A 2D numpy array of data points stored row-wise (n x d)\n",
    "        y: A 1D numpy array of target values (n x 1)\n",
    "        w: A 1D numpy array of weights (d x 1)\n",
    "        reg: Optional regularization parameter\n",
    "    Returns:\n",
    "        Learned weight vector (d x 1)\n",
    "    \"\"\"\n",
    "    if w is None:\n",
    "        w = np.zeros((X.shape[1], 1))\n",
    "        \n",
    "    bestw = w\n",
    "    bestcost, grad = log_cost(X, y, w)\n",
    "    bestcost = bestcost*10\n",
    "    for itercount in range(0, maxiter):\n",
    "        cost, grad = log_cost(X, y, w, reg=reg)\n",
    "        w = w - stepsize*grad\n",
    "        if cost < bestcost:\n",
    "            bestcost = cost\n",
    "            bestw = w\n",
    "    return bestw, bestcost\n",
    "\n",
    "def mini_batch_grad_descent(X, y, w=None, reg=0, batchsize=2*6, epochs=2*6, stepsize=0.01):\n",
    "    \"\"\"\n",
    "    Run Mini-Batch Gradient Descent on data X,y to minimize the NLL for logistic regression on data X,y\n",
    "    The input defines mini-batch size and the number of passess through the data set (epochs)\n",
    "\n",
    "    Args:\n",
    "        X: A 2D numpy array of data points stored row-wise (n x d)\n",
    "        y: A 1D numpy array of target values (n x 1)\n",
    "        w: A 1D numpy array of weights (d x 1)\n",
    "        reg: Optional regularization parameter\n",
    "        batchsize: Number of data points to use in each batch\n",
    "        epochs: Number of times to go over all data points\n",
    "    Returns:\n",
    "        Learned weight vector (d x 1)\n",
    "    \"\"\"\n",
    "    if w is None:\n",
    "        w = np.zeros((X.shape[1], 1))\n",
    "    \n",
    "    # Please implement me and use \"batchsize\" data points in each gradient computation!\n",
    "    n = X.shape[0]\n",
    "    if n % batchsize != 0:\n",
    "        print('Some datapoints are discarded every epoch to achieve the specified batchsize.')\n",
    "        print(n)\n",
    "    bestw = w\n",
    "    bestcost, grad = log_cost(X, y, w)\n",
    "    epochcount = 0\n",
    "    for epoch in range(0,epochs):\n",
    "        idx = np.random.permutation(n)\n",
    "        for batch in range(0,n//batchsize):\n",
    "            # a batchsize-slice of the randomly permutation and run grad_descent (only one iteration!) \n",
    "            idxb = idx[epoch*batchsize:(epoch+1)*batchsize]\n",
    "            w, cost = batch_grad_descent(X[idxb,:], y[idxb], bestw, reg=reg, stepsize=stepsize, maxiter=1,)\n",
    "            #w, cost = batch_grad_descent(X, y, w, reg=reg, stepsize=stepsize, maxiter=2**8,)\n",
    "        cost, grad = log_cost(X, y, w)\n",
    "        if cost < bestcost:\n",
    "            bestcost = cost\n",
    "            bestw = w\n",
    "        print('          NLL: ', cost, 'after ', epoch+1, ' epochs.')\n",
    "    return bestw, bestcost\n",
    "\n",
    "def main():\n",
    "    \"\"\"An example main method. You should replace this with whatever you need.\"\"\"\n",
    "    print('You rang sir/miss')\n",
    "    digits, labels = load_data()\n",
    "    idx1 = (labels == 2)\n",
    "    idx2 = (labels == 7)\n",
    "    img27 = digits[idx1 | idx2,:]\n",
    "    lab27 = labels[idx1 | idx2]\n",
    "    lab27[lab27==2] = 0\n",
    "    lab27[lab27==7] = 1\n",
    "    \n",
    "    ## code for plotting and checking the logistic()-function\n",
    "    #x = np.arange(-20,20,0.5)\n",
    "    #plt.scatter(x,logistic(x))\n",
    "    \n",
    "    ## How does the all-zero vector perform:\n",
    "    cost, grad = log_cost(img27, lab27)\n",
    "    print('    Start NLL: ', cost)\n",
    "    \n",
    "    ## Try the batch_grad_descent\n",
    "    print('batch_grad_descent')\n",
    "    w, cost = batch_grad_descent(img27, lab27)\n",
    "    print('    Final NLL: ', cost)\n",
    "    \n",
    "    ## Try the mini_batch_grad_descent\n",
    "    ## fejl, skal fejlsøges!\n",
    "    print('mini_batch_grad_descent')\n",
    "    w, cost = mini_batch_grad_descent(img27, lab27)\n",
    "    print('    Final NLL: ', cost)\n",
    "    \n",
    "    # print(w)\n",
    "    #plt.imshow(w.reshape(28, 28), cmap='bone')\n",
    "    #print('Look look it is a pretty two i think')\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deliverables\n",
    "* Implement batch and mini-batch gradient descent for logistic regression.\n",
    "* Test both gradient descent implementations on images of twos and sevens and compute the number of misclassifications. You should train on data from the training set and test on the test set.\n",
    "* Implement a full OCR classifier with logistic regression using the all-vs-one technique. Here you are free to choose between full-batch and mini-batch gradient descent\n",
    "* Bonus Exercise: Add regularization to the cost function and see if you can improve the performance (remember not to apply regularization to the bias parameter)\n",
    "* Bonus Exercise: Test your implementation on the MNIST digits and compare the results to the AU Digits\n",
    "\n",
    "Your code file must follow the given layout with functions log_grad, batch_grad_descent, mini_batch_gradient_descent that must implement the interface described in the functions descriptions. The remaining part you can design any way you desire.\n",
    "\n",
    "Besides your Python file, save the best parameters you have found in a file with \n",
    "<tt>np.savez(\"params.npz\", theta=best_theta)</tt> and hand it in. The parameters should be optimized for the auDigits data set classifying whether a digit is 2 or a 7.\n",
    "\n",
    "Note that the file should not execute any functions when imported as a module, so you should wrap the call to your main function in <tt>if \\_\\_name\\_\\_ == \"\\_\\_main\\_\\_\"</tt>. For more information see the Python documentation on \\_\\_main\\_\\_.\n",
    "Save the files in a zip archive and upload it to Blackboard with your report.\n",
    "\n",
    "## Report\n",
    "In your report, you should shortly explain your algorithms and the choices you have made.\n",
    "You should include a plot of some of the digits that your classifier makes mistakes on -- and discuss why that may be.\n",
    "\n",
    "To show the performance of your gradient descent implementation include a plot of the cost function as a function\n",
    "of the number of steps taken. You should compare the running time and the convergence/output quality of mini-batch and full-batch gradient descent.\n",
    "\n",
    "You should provide a figure that shows the parameter vectors for the \n",
    "case of classifying 2s versus 7s, and for the 10 all vs.~one classifiers.\n",
    "Furthermore, your report should include results for the pairwise\n",
    "computations (at least two vs seven) as well as the results for the full classifier on the AU data set.\n",
    "\n",
    "If you add regularization, include the results for the regularized versions of the cost function and gradient and\n",
    "compare the result with the unregularized implementation.\n",
    "\n",
    "Furthermore you must answer the following theoretical questions\n",
    "(although the bonus question is optional).\n",
    "\n",
    "\n",
    "* Sanity Check:\n",
    "  What happens if we randomly permute the pixels in each image (with the\n",
    "  same permutation) before we train the classifier? Will we get a\n",
    "  classifier that is better, worse, or the same? Give a short explanation.\n",
    "\n",
    "* Linear Separable:\n",
    "  If the data is linearly separable, what happens\n",
    "  to weights when we implement logistic regression with gradient\n",
    "  descent? That is, how do the weights that minimize the negative log likelihood look like?\n",
    "  Assume that we have full precision (that is, ignore floating point errors).\n",
    "  We can run gradient descent on the data set for as long as we want (suppose God helps you).\n",
    "  Now what will happen with the weights in the limit?\n",
    "  Do they converge to some fixed number (fluctuate around it) or do they\n",
    "  keep increasing in magnitude (absolute value)?\n",
    "  Give a short explanation for your answer.\n",
    "  What happens if we add regularization?\n",
    "\n",
    "* Bonus Question: Convexity of negative log likelihood\n",
    "  Show that the negative log likelihood function for logistic regression is convex.\n",
    "  Is it still convex if we add regularization?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multinomial/Softmax  Regression\n",
    "\n",
    "In this exercise we generalize logistic regression to handle $K$\n",
    "classes instead of 2. This is known as Multinomial or Softmax\n",
    "regression and we will use the exact same approach as for logistic\n",
    "regression, but it becomes a little more technical due to the extra\n",
    "classes.\n",
    "\n",
    "In Softmax regression we are classifying into $K$ classes (instead\n",
    "of 2 for logistic regression).  We encode the target values, $y$, as a vector of\n",
    "length $K$ with all zeros except one which corresponds to the class (one-in-K encoding).\n",
    "If an example belong to class 3 and there are five classes then\n",
    "$y = [0,0,1,0,0]^\\intercal = e_3$.\n",
    "\n",
    "So $Y$ is now a matrix of size $n \\times K$, and the data matrix $X$ is unchanged.\n",
    "\n",
    "We can no longer use the logistic function as the probability function.\n",
    "Instead we use a generalization which is the *softmax* function.\n",
    "Softmax takes as input a vector of length $K$\n",
    "and outputs another vector of the same length $K$,\n",
    "that is a mapping from the $K$ input numbers into $K$\n",
    "*probabilities*, e.g. they sum to one.  Softmax is defined as\n",
    "$$\n",
    "\\textrm{softmax}(x)_j =\n",
    "\\frac{e^{x_j}}\n",
    "{\\sum_{i=1}^K e^{x_i}}\n",
    "\\textrm{ for } j = 1, \\dots, K.\n",
    "$$\n",
    "\n",
    "Notice that the denominator acts as a normalization term that ensures\n",
    "that the probabilities sum to one. Again we get nice derivatives\n",
    "(which we like),\n",
    "$$\n",
    "\\frac\n",
    "{\\partial \\textrm{softmax}(x)_i}\n",
    "{\\partial x_j} =\n",
    "(\\delta_{i,j} - \\textrm{softmax}(x)_j)\n",
    "\\textrm{softmax}(x)_i,\n",
    "$$\n",
    "where $\\delta_{i,j} = 1$ if $i = j$ and 0 otherwise.\n",
    "Exactly as before this is a linear model for each class. The parameter set\n",
    "$\\theta$ is represented as a $(d+1) \\times K$ matrix giving $d+1$\n",
    "parameters (+1 for the bias) for each of the $K$ classes (parameters\n",
    "for class $c$ is column $c$), meaning that\n",
    "$\\theta=[\\theta_1,\\ldots,\\theta_K]$. We get\n",
    "\n",
    "$$\n",
    "p(y \\mid x,\\theta) =\n",
    "\\textrm{softmax}(\\theta^\\intercal x) =\n",
    " \\left \\{\n",
    "\\begin{array}{l l}\n",
    " \\textrm{softmax}(\\theta^\\intercal x)_1 & \\text{ if } y = e_1,  \\\\\n",
    " \\vdots & \\\\\n",
    " \\textrm{softmax}(\\theta^\\intercal x)_K & \\text { if } y = e_K.\n",
    "\\end{array}\n",
    "\\right.\n",
    "$$\n",
    "\n",
    "Think of the probability distribution over $y$ as throwing a $K$-sided die\n",
    "where the likelihood of landing on each of the $K$ sides is stored in the\n",
    "vector $\\textrm{softmax}(\\theta^\\intercal x)$ (which is a vector of length $K$) and the\n",
    "probability of landing on side $i$ is $\\textrm{softmax}(\\theta^\\intercal x)_i$. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "As for logistic regression we compute the likelihood of the data given a fixed matrix of parameters.\n",
    "We use the notation $[z]$ for the indicator function e.g. $[z]$ is one if $z$ is true.\n",
    "\n",
    "$$\n",
    "P(D \\mid \\theta) =\n",
    "\\prod_{(x,y)\\in D}\n",
    "\\prod_{j=1}^K\n",
    "\\textrm{softmax}(\\theta^\\intercal x)_j^{[y_j=1]}\n",
    "=\n",
    "\\prod_{(x,y)\\in D}\n",
    "y^\\intercal\n",
    "\\textrm{softmax}(\\theta^\\intercal x)\n",
    ".\n",
    "$$\n",
    "\n",
    "This way of expressing is the same as we did for logistic regression.\n",
    "The product over the $K$ classes will have one element\n",
    "that is not one namely the $y_j$'th element ($y$ is a\n",
    "vector of $K-1$ zeros and a one). The remaining probabilities are\n",
    "raised to a power of zero and has the value one.\n",
    "\n",
    "For convenience we minimize the negative log likelihood of the data\n",
    "instead of maximizing the likelihood of the data and get a pointwise sum.\n",
    "\n",
    "$$\n",
    "\\textrm{NLL}(D\\mid \\theta) =\n",
    "-\\sum_{(x,y)\\in D}\n",
    "\\sum_{j=1}^K\n",
    "[y_j=1]\n",
    "\\ln (\\textrm{softmax}(\\theta^\\intercal x)_j)\n",
    "=\n",
    "-\\sum_{(x,y)\\in D}\n",
    "y^\\intercal\n",
    "\\ln (\\textrm{softmax}(\\theta^\\intercal x))\n",
    ".\n",
    "$$\n",
    "\n",
    "Again we define $E_\\textrm{in} = \\frac{1}{|D|} \\textrm{NLL}$ and as for logistic regression this is a convex function which cannot be solved for a zero analytically.  To apply gradient descent as before all you really need is the\n",
    "gradient of the negative log likelihood function.  This gradient is a\n",
    "*simple* generalization of the one used in logistic\n",
    "regression. There is a set of parameters for each of $K$ classes, $\\theta_j$ for\n",
    "$j=1,\\ldots,K$ (the $j$'th column in the parameter matrix $\\theta$) that must be learned.\n",
    "Luckily some nice people tell you what it is:\n",
    "$$\n",
    "\\nabla \\textrm{NLL}(\\theta) =\n",
    "-X^\\intercal\n",
    "(Y - \\textrm{softmax}(X\\theta)),\n",
    "$$\n",
    "\n",
    "where softmax is taken on each row of the matrix (that is, $X \\theta$ is an\n",
    "$n \\times K$ matrix and softmax is computed for each training case over\n",
    "the $K$ classes).\n",
    "\n",
    "You should, of course, verify this yourself but you do not have to\n",
    "prove it. \n",
    "\n",
    "\n",
    "\n",
    "### Numerical Issues with Softmax\n",
    "There are some numerical issues with the softmax function\n",
    "\n",
    "$$\n",
    "\\textrm{softmax}(x)_j = \\frac{e^{x_j}}{\\sum_{i=1}^K e^{x_i}} \\textrm{ for } j=1,\\ldots,K.\n",
    "$$\n",
    "\n",
    "This is because this is a sum of exponentials (before taking logs again),\n",
    "and exponentiation of numbers tend to make them very large giving numerical problems.\n",
    "Let's look at the function for a fixed $j$,\n",
    "$\\frac{e^{x_j}}{\\sum_{i=1}^K e^{x_i}}$.\n",
    "Since the logarithm and the exponential function are each other's inverse,\n",
    "we may write it as\n",
    "$$\n",
    "e^{\\textstyle x_j - \\ln(\\sum_{i=1}^K e^{x_i})}\n",
    "$$\n",
    "\n",
    "The problematic part is the logarithm of the sum of exponentials.\n",
    "However, we can move $e^c$ for any constant $c$ outside the sum easily, that is,\n",
    "$$\n",
    "\\ln\\left(\\sum_i e^{x_i}\\right) =\n",
    "\\ln\\left(e^c \\sum_i e^{x_i-c}\\right) =\n",
    "c + \\ln\\left(\\sum_i e^{x_i -c}\\right).\n",
    "$$\n",
    "\n",
    "We need to find a good $c$, and we choose $c = \\max_i x_i$ since\n",
    "$e^{x_i}$ is the dominant term in the sum. We are less concerned with\n",
    "values being inadvertently rounded to zero since that does not\n",
    "change the value of the sum significantly.\n",
    "\n",
    "With this in place you should be able to make a gradient descent\n",
    "implementation for softmax regression, like you did for logistic\n",
    "regression.  We have again provied a very similar code structure for you to follow.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def softmax(X):\n",
    "    \"\"\" \n",
    "    Compute the softmax of each row of an input matrix (2D numpy array).\n",
    "    the numpy functions amax, log, exp, sum may come in handy as well as the keepdims=True option.\n",
    "    Remember to handle the numerical problems as discussed above.\n",
    "    \n",
    "    Args:\n",
    "        X: A 2D numpy array\n",
    "    Returns:\n",
    "        A new 2D numpy array of the same size as the input where each row in the input \n",
    "        has been replaced by the softmax of that row\n",
    "    \"\"\"\n",
    "    res = X  # Replace me!\n",
    "    assert res.shape == X.shape\n",
    "    return res\n",
    "    \n",
    "    \n",
    "def soft_cost(X, Y, W, reg=0):\n",
    "    \"\"\" \n",
    "    Compute the regularized cross entropy and the gradient under the logistic regression model \n",
    "    using data X,y and weight vector w.\n",
    "\n",
    "    Args:\n",
    "        X: A 2D numpy array of data points stored row-wise (n x d)\n",
    "        Y: A 2D numpy array of target values in 1-in-K encoding (n x K)\n",
    "        W: A 2D numpy array of weights (d x K)\n",
    "        reg: Optional regularization parameter\n",
    "    Returns:\n",
    "        totalcost: Average Negative Log Likelihood of w\n",
    "        gradient: The gradient of the average Negative Log Likelihood at w\n",
    "    \"\"\"\n",
    "    d, K = W.shape\n",
    "    n = X.shape[0]\n",
    "    cost = 0\n",
    "    grad = np.zeros((d, K))\n",
    "    return cost, grad\n",
    "\n",
    "def batch_grad_descent(X, Y, W=None, reg=0.0):\n",
    "    \"\"\"\n",
    "    Run batch gradient descent to learn softmax regression weights that minimize the in-sample error\n",
    "    Args:\n",
    "        X: A 2D numpy array of data points stored row-wise (n x d)\n",
    "        Y: A 2D numpy array of target values in 1-in-K encoding (n x K)\n",
    "        W: A 2D numpy array of weights (d x K)\n",
    "        reg: Optional regularization parameter\n",
    "    Returns:\n",
    "        The learned weights      \n",
    "    \"\"\"\n",
    "    if W is None:\n",
    "        W = np.zeros((X.shape[1], y.shape[1]))\n",
    "    # Please implement me and use all data in each gradient computation!\n",
    "    return W\n",
    "\n",
    "def mini_batch_grad_descent(X, Y, reg=0.0, W=None, batchsize=16, epochs=10):\n",
    "    \"\"\"\n",
    "    Run Mini-Batch Gradient Descent on data X,Y to minimize the NLL for logistic regression on data X,y\n",
    "    Args:\n",
    "        X: A 2D numpy array of data points stored row-wise (n x d)\n",
    "        Y: A 2D numpy array of target values in 1-in-K encoding (n x K)\n",
    "        W: A 2D numpy array of weights (d x K)\n",
    "        reg: Optional regularization parameter\n",
    "        batchsize: size of mini-batch\n",
    "        epochs: Number of iterations through the data to use\n",
    "    Returns:\n",
    "        The learned weights     \n",
    "    \"\"\"\n",
    "    if W is None:\n",
    "        W = np.zeros((X.shape[1], Y.shape[1]))\n",
    "    # Please implement me and use \"batchsize\" data points in each gradient computation!\n",
    "    return W\n",
    "\n",
    "def test_softmax():\n",
    "    \"\"\"A trivial test of the softmax function\"\"\"\n",
    "    X = np.ones((3, 3))\n",
    "    X[0,0] = 2\n",
    "    X[2,0] = 0\n",
    "    X[2,1] = 0\n",
    "    sm = softmax(X)\n",
    "    print('X =\\n%s\\nsoftmax(X) =\\n%s' % (X, sm))\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    test_softmax()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Deliverables\n",
    "* Implement batch and mini-batch gradient descent for softmax regression and apply it for classifying input digits.\n",
    "* Train it on the AU training set and test on the test set. \n",
    "* Test your implementation on the MNIST data set and compare the results with the result from the AU data set.\n",
    "* Bonus Exercise: Add regularization to your implementation and compare\n",
    "\n",
    "As for logistic regression, your code file must follow the given layout with functions soft_cost, batch_grad_descent, mini_batch_gradient_descent that must implement the interface described in the functions descriptions.\n",
    "Besides your Python file, save the best parameters you have found in a file with <tt>np.savez(\"params.npz\", theta=best_theta)</tt> and hand it in. The parameters should be optimized for the auDigits data set. \n",
    "\n",
    "Note that the file should not execute any functions when imported as a module, so you should wrap the call to your main function in <tt>if \\__name\\__ == \"\\__main\\__\"</tt>. For more information see the Python documentation on <tt>\\__main\\__</tt>.\n",
    "\n",
    "\n",
    "## Report\n",
    "\n",
    "In your report, you should shortly explain your algorithms and the choices you have made.\n",
    "You should plot some of the digits that your classifiers make mistakes on\n",
    "and discuss why this may be.\n",
    "Your report should include a plot of the cost function as a function\n",
    "of the iteration for say 50 iterations. You should compare the running time and the convergence of mini-batch and full-batch gradient descent.\n",
    "You should also provide a figure that shows the plot of the parameter vectors for your best classifier.\n",
    "Your report should include your results on the test set and they should be compared to the results from logistic regression\n",
    "\n",
    "If you add regularization, include the results for the regularized versions of the cost function and gradient and\n",
    "compare the result with the unregularized implementation.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
